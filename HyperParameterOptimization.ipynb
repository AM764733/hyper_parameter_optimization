{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import modules from sklearn and load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
       "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
       "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
       "       'worst concave points', 'worst symmetry', 'worst fractal dimension',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer = datasets.load_breast_cancer()\n",
    "data = pd.DataFrame(breast_cancer.data[:, :],columns = breast_cancer.feature_names)\n",
    "data['target'] = breast_cancer.target\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
    "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
    "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
    "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
    "       'smoothness error', 'compactness error', 'concavity error',\n",
    "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
    "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
    "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
    "       'worst concave points', 'worst symmetry', 'worst fractal dimension']]\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, \n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score (test):  0.956140350877193\n",
      "f1_score (train):  0.9978021978021978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score (test): \" , f1_score(rf.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score (train): \" , f1_score(rf.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion': ['entropy', 'gini'],\n",
    "        'max_depth': list(np.linspace(10, 600, 10, dtype = int)) + [None],\n",
    "        'max_features': ['auto', 'sqrt','log2', None],\n",
    "        'min_samples_leaf': [1, 4, 6, 8, 12],\n",
    "        'min_samples_split': [2, 5, 7, 10, 14],\n",
    "        'n_estimators': list(np.linspace(100, 1000, 10, dtype = int))}\n",
    "\n",
    "model = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=params,\n",
    "                           n_iter=50, scoring='f1', verbose=3, cv=3, n_jobs=-1,\n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   22.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   param_distributions={'criterion': ['entropy', 'gini'],\n",
       "                                        'max_depth': [10, 75, 141, 206, 272,\n",
       "                                                      337, 403, 468, 534, 600,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt', 'log2',\n",
       "                                                         None],\n",
       "                                        'min_samples_leaf': [1, 4, 6, 8, 12],\n",
       "                                        'min_samples_split': [2, 5, 7, 10, 14],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 400,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': None,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score (test):  0.9649122807017544\n",
      "f1_score (train):  1.0\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**model.best_params_,\n",
    "                            random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score (test): \" , f1_score(rf.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score (train): \" , f1_score(rf.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'criterion': ['entropy'],\n",
    "        'max_depth': [395, 397, 400, 403],\n",
    "        'max_features': ['sqrt'],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'min_samples_split': [4, 5, 6],\n",
    "        'n_estimators': [580, 600, 620]}\n",
    "\n",
    "model2 = GridSearchCV(estimator=RandomForestClassifier(), param_grid=grid,\n",
    "                      scoring='f1', verbose=3, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:   28.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'criterion': ['entropy'],\n",
       "                         'max_depth': [395, 397, 400, 403],\n",
       "                         'max_features': ['sqrt'], 'min_samples_leaf': [1, 2],\n",
       "                         'min_samples_split': [4, 5, 6],\n",
       "                         'n_estimators': [580, 600, 620]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 395,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 4,\n",
       " 'n_estimators': 580}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score (test):  0.9649122807017544\n",
      "f1_score (train):  1.0\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**model2.best_params_,\n",
    "                            random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score (test): \" , f1_score(rf.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score (train): \" , f1_score(rf.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.956140350877193\n",
      "f1_score:  1.0\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'loss': ['deviance', 'exponential'],\n",
    "         'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
    "         'n_estimators': list(np.linspace(30, 100, 7, dtype=int)),\n",
    "         'subsample': [0.5, 0.7, 0.9 ,1],\n",
    "         'min_samples_split': [3, 4, 5, 6, 7],\n",
    "         'min_samples_leaf': list(np.linspace(1, 100, 10, dtype=int)),\n",
    "         'max_depth': [2,4,6,8,10],\n",
    "         'max_features': ['sqrt'],\n",
    "         }\n",
    "\n",
    "model = RandomizedSearchCV(estimator=GradientBoostingClassifier(random_state=42), \n",
    "                           param_distributions=params, scoring='f1', n_iter=300, \n",
    "                           verbose=3, cv=3, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                        init=None,\n",
       "                                                        learning_rate=0.1,\n",
       "                                                        loss='deviance',\n",
       "                                                        max_depth=3,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_i...\n",
       "                   param_distributions={'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'loss': ['deviance', 'exponential'],\n",
       "                                        'max_depth': [2, 4, 6, 8, 10],\n",
       "                                        'max_features': ['sqrt'],\n",
       "                                        'min_samples_leaf': [1, 12, 23, 34, 45,\n",
       "                                                             56, 67, 78, 89,\n",
       "                                                             100],\n",
       "                                        'min_samples_split': [3, 4, 5, 6, 7],\n",
       "                                        'n_estimators': [30, 41, 53, 65, 76, 88,\n",
       "                                                         100],\n",
       "                                        'subsample': [0.5, 0.7, 0.9, 1]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.7,\n",
       " 'n_estimators': 65,\n",
       " 'min_samples_split': 3,\n",
       " 'min_samples_leaf': 34,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 10,\n",
       " 'loss': 'deviance',\n",
       " 'learning_rate': 0.2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9824561403508771\n",
      "f1_score:  1.0\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(**model.best_params_, \n",
    "                                random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'loss': ['deviance'],\n",
    "         'learning_rate': [0.175, 0.2, 0.225],\n",
    "         'n_estimators': [60, 65, 70],\n",
    "         'subsample': [0.6, 0.7, 0.8],\n",
    "         'min_samples_split': [2, 3, 4],\n",
    "         'min_samples_leaf': [30, 34, 40],\n",
    "         'max_depth': [9,10,11],\n",
    "         'max_features': ['sqrt'],\n",
    "         }\n",
    "\n",
    "model2 = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42), param_grid=grid,\n",
    "                      scoring='f1', verbose=3, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 729 candidates, totalling 2187 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 592 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2187 out of 2187 | elapsed:   17.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                  init=None, learning_rate=0.1,\n",
       "                                                  loss='deviance', max_depth=3,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  n_estimators=100,\n",
       "                                                  n_iter_no...\n",
       "                                                  verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.175, 0.2, 0.225],\n",
       "                         'loss': ['deviance'], 'max_depth': [9, 10, 11],\n",
       "                         'max_features': ['sqrt'],\n",
       "                         'min_samples_leaf': [30, 34, 40],\n",
       "                         'min_samples_split': [2, 3, 4],\n",
       "                         'n_estimators': [60, 65, 70],\n",
       "                         'subsample': [0.6, 0.7, 0.8]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 9,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 30,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 70,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9736842105263158\n",
      "f1_score:  1.0\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(**model2.best_params_, \n",
    "                                random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.quniform('max_depth', 10, 1000, 10),\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.choice('min_samples_leaf', [10, 20, 25, 30, 35, 40, 45, 50, 55, 60]),\n",
    "        'min_samples_split' : hp.choice('min_samples_split', [2, 3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "        'n_estimators' : hp.choice('n_estimators', [100, 150, 200, 250, 300, 350, 400, 450, 500, \n",
    "                                                    550, 600, 650, 700, 750, 800, 850, 900, 950,\n",
    "                                                    1000])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = space['n_estimators'], \n",
    "                                 )\n",
    "    \n",
    "    accuracy = cross_val_score(model, X_train, y_train, cv=4).mean()\n",
    "\n",
    "    # We aim to maximize accuracy, therefore we return it as a negative value\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [02:39<00:00,  1.99s/it, best loss: -0.9560031523595445]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 0,\n",
       " 'max_depth': 370.0,\n",
       " 'max_features': 0,\n",
       " 'min_samples_leaf': 0,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = 80,\n",
    "            trials= trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score (test):  0.9649122807017544\n",
      "f1_score (train):  0.9758241758241758\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(criterion='entropy',max_depth=370, max_features='auto',\n",
    "                            min_samples_leaf=10, min_samples_split=7, n_estimators=600,\n",
    "                            random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score (test): \" , f1_score(rf.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score (train): \" , f1_score(rf.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
