{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import modules from sklearn and load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Height', 'Weight'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/luken2/Documents/GitHub/hyper-parameter-tuning/data/height_weight_gender.csv')\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_class = []\n",
    "for gender in df['Gender']:\n",
    "    if gender == 'Male':\n",
    "        binary_class.append(1)\n",
    "    if gender == 'Female':\n",
    "        binary_class.append(0)\n",
    "df['target'] = pd.DataFrame(binary_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Height', 'Weight']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, \n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9045\n",
      "f1_score:  0.99175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(rf.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(rf.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulating a randomized search we need 4 things:\n",
    "1. Estimator: Model to be optimized\n",
    "2. param_distribution: space of params to randomly search\n",
    "3. n_iter: number of iterations to perform\n",
    "4. scoring: metric to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion': ['entropy', 'gini'],\n",
    "        'max_depth': list(np.linspace(10, 600, 10, dtype = int)) + [None],\n",
    "        'max_features': ['auto', 'sqrt','log2', None],\n",
    "        'min_samples_leaf': [1, 4, 6, 8, 12],\n",
    "        'min_samples_split': [2, 5, 7, 10, 14],\n",
    "        'n_estimators': list(np.linspace(100, 1000, 10, dtype = int))}\n",
    "\n",
    "model = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=params,\n",
    "                           n_iter=50, scoring='f1', verbose=3, cv=3, n_jobs=-1,\n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   param_distributions={'criterion': ['entropy', 'gini'],\n",
       "                                        'max_depth': [10, 75, 141, 206, 272,\n",
       "                                                      337, 403, 468, 534, 600,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt', 'log2',\n",
       "                                                         None],\n",
       "                                        'min_samples_leaf': [1, 4, 6, 8, 12],\n",
       "                                        'min_samples_split': [2, 5, 7, 10, 14],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `.best_params_` will give you the parameters it found and can be plugged directly into the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 800,\n",
       " 'min_samples_split': 14,\n",
       " 'min_samples_leaf': 12,\n",
       " 'max_features': 'log2',\n",
       " 'max_depth': 141,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.925\n",
      "f1_score:  0.923375\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**model.best_params_,\n",
    "                            random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(rf.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(rf.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulating a grid search we need 3 things:\n",
    "1. Estimator: Model to be optimized\n",
    "2. param_grid: Grid of parameters to be tested (Avoid Large Grids!)\n",
    "3. scoring: Metric to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'criterion': ['entropy'],\n",
    "        'max_depth': [70, 80, 90, 100],\n",
    "        'max_features': ['log2'],\n",
    "        'min_samples_leaf': [13],\n",
    "        'min_samples_split': [9, 10, 11],\n",
    "        'n_estimators': [830]}\n",
    "\n",
    "model2 = GridSearchCV(estimator=RandomForestClassifier(), param_grid=grid,\n",
    "                      scoring='f1', verbose=3, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  36 | elapsed:   26.0s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:   26.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'criterion': ['entropy'],\n",
       "                         'max_depth': [70, 80, 90, 100],\n",
       "                         'max_features': ['log2'], 'min_samples_leaf': [13],\n",
       "                         'min_samples_split': [9, 10, 11],\n",
       "                         'n_estimators': [830]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 100,\n",
       " 'max_features': 'log2',\n",
       " 'min_samples_leaf': 13,\n",
       " 'min_samples_split': 11,\n",
       " 'n_estimators': 830}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9245\n",
      "f1_score:  0.923375\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(**model2.best_params_,\n",
    "                            random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(rf.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(rf.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulating a bayesian optimization problem in hyperopt we need 4 parts:\n",
    "1. Objective Function: Takes in an input and returns a loss to minimize\n",
    "2. Space: Range of values to test\n",
    "3. Optimization Algorithm: Method to construct the surragate function and choose next values to evaluate\n",
    "4. Results: Score, Value pairs algorithm used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperopt library\n",
    "`pip install hyperopt`\n",
    "* hp: Gives functions to create probability distibutions of our range of space\n",
    "* fmin: Minimizes objective function given range of values to test and optimization algorithm\n",
    "* tpe: Optimization algorithm (Tree-Structured Partizan Estimator)\n",
    "* STATUS_OK: Checks status?\n",
    "* Trials: Results history if we want to see what is going on behind the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make parameter space use hyperopt.hp\n",
    "1. `hp.choice` : Uniform distribution over each value specified\n",
    "2. `hp.uniform` : Continuous uniform distribution (Floats)\n",
    "3. `hp.quniform` : Discrete uniform distribution (Integers\n",
    "4. `hp.loguniform` : Countinuous log distribution (Floats)\n",
    "5. `hp.qloguniform` : Discrete log distribution (Integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.quniform('max_depth', 10, 200, 5),\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.choice('min_samples_leaf', range(2,20)),\n",
    "        'min_samples_split': hp.choice('min_samples_split', range(2,50)),\n",
    "        'n_estimators': hp.choice('n_estimators', range(100, 1000, 10))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \n",
    "    # Model to be hyper-optimized\n",
    "    \n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    \n",
    "    # Score to be used in evaluation\n",
    "    \n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro').mean()\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    \n",
    "    loss = 1 - score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [10:07<00:00, 12.16s/it, best loss: 0.08225312228074633]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 1,\n",
       " 'max_depth': 180.0,\n",
       " 'max_features': 1,\n",
       " 'min_samples_leaf': 11,\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 30}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.08050118811690454,\n",
       " 'params': {'learning_rate': 0.011273254429806057,\n",
       "  'loss': 'exponential',\n",
       "  'max_depth': 4.0,\n",
       "  'max_features': 'auto',\n",
       "  'min_samples_leaf': 60,\n",
       "  'min_samples_split': 8,\n",
       "  'n_estimators': 470,\n",
       "  'subsample': 0.5181493292615545},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.924\n",
      "f1_score:  0.923375\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(criterion='gini', max_depth=180, max_features='sqrt',\n",
    "                            min_samples_leaf=13, min_samples_split=12, n_estimators=400,\n",
    "                            random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(rf.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(rf.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9235\n",
      "f1_score:  0.925\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'loss': ['deviance', 'exponential'],\n",
    "         'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
    "         'n_estimators': list(np.linspace(30, 300, 7, dtype=int)),\n",
    "         'subsample': [0.5, 0.7, 0.9 ,1],\n",
    "         'min_samples_split': [3, 4, 5, 6, 7],\n",
    "         'min_samples_leaf': list(np.linspace(1, 100, 10, dtype=int)),\n",
    "         'max_depth': [2,4,6,8,10],\n",
    "         'max_features': ['auto', 'sqrt', 'log2'],\n",
    "         }\n",
    "\n",
    "model = RandomizedSearchCV(estimator=GradientBoostingClassifier(random_state=42), \n",
    "                           param_distributions=params, scoring='f1', n_iter=50, \n",
    "                           verbose=3, cv=3, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   17.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                        init=None,\n",
       "                                                        learning_rate=0.1,\n",
       "                                                        loss='deviance',\n",
       "                                                        max_depth=3,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_i...\n",
       "                                        'loss': ['deviance', 'exponential'],\n",
       "                                        'max_depth': [2, 4, 6, 8, 10],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 12, 23, 34, 45,\n",
       "                                                             56, 67, 78, 89,\n",
       "                                                             100],\n",
       "                                        'min_samples_split': [3, 4, 5, 6, 7],\n",
       "                                        'n_estimators': [30, 75, 120, 165, 210,\n",
       "                                                         255, 300],\n",
       "                                        'subsample': [0.5, 0.7, 0.9, 1]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.7,\n",
       " 'n_estimators': 165,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 56,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 2,\n",
       " 'loss': 'deviance',\n",
       " 'learning_rate': 0.05}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9235\n",
      "f1_score:  0.920875\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(**model.best_params_, \n",
    "                                random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'loss': ['deviance'],\n",
    "         'learning_rate': [0.05, 0.1],\n",
    "         'n_estimators': [100, 150],\n",
    "         'subsample': [0.7, 0.8],\n",
    "         'min_samples_split': [5],\n",
    "         'min_samples_leaf': [40, 50, 60],\n",
    "         'max_depth': [2, 3, 4],\n",
    "         'max_features': ['sqrt'],\n",
    "         }\n",
    "\n",
    "model2 = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42), param_grid=grid,\n",
    "                      scoring='f1', verbose=3, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:   11.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                  init=None, learning_rate=0.1,\n",
       "                                                  loss='deviance', max_depth=3,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  n_estimators=100,\n",
       "                                                  n_iter_no...\n",
       "                                                  validation_fraction=0.1,\n",
       "                                                  verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.05, 0.1], 'loss': ['deviance'],\n",
       "                         'max_depth': [2, 3, 4], 'max_features': ['sqrt'],\n",
       "                         'min_samples_leaf': [40, 50, 60],\n",
       "                         'min_samples_split': [5], 'n_estimators': [100, 150],\n",
       "                         'subsample': [0.7, 0.8]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 3,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 60,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 100,\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9225\n",
      "f1_score:  0.92275\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(**model2.best_params_, \n",
    "                                random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_train), y_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {'loss': hp.choice('loss', ['deviance', 'exponential']),\n",
    "         'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "         'n_estimators': hp.choice('n_estimators', range(50,1000,10)),\n",
    "         'subsample': hp.uniform('subsample', 0.4, 1),\n",
    "         'min_samples_split': hp.choice('min_samples_split', range(2, 20, 2)),\n",
    "         'min_samples_leaf': hp.choice('min_samples_leaf', range(10,100,5)),\n",
    "         'max_depth': hp.quniform('max_depth', 2, 12, 1),\n",
    "         'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2'])\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \n",
    "    model = GradientBoostingClassifier(**params, random_state=42, verbose=0)\n",
    "    \n",
    "    score = cross_val_score(model, X, y, cv=5, scoring='f1_macro').mean()\n",
    "    \n",
    "    loss = 1 - score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:57<00:00,  9.54s/it, best loss: 0.08050118811690454]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.011273254429806057,\n",
       " 'loss': 1,\n",
       " 'max_depth': 4.0,\n",
       " 'max_features': 0,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 42,\n",
       " 'subsample': 0.5181493292615545}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpe.algorithm = tpe.suggest\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.08050118811690454,\n",
       " 'params': {'learning_rate': 0.011273254429806057,\n",
       "  'loss': 'exponential',\n",
       "  'max_depth': 4.0,\n",
       "  'max_features': 'auto',\n",
       "  'min_samples_leaf': 60,\n",
       "  'min_samples_split': 8,\n",
       "  'n_estimators': 470,\n",
       "  'subsample': 0.5181493292615545},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.9235\n",
      "f1_score:  0.921125\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(learning_rate=0.011, loss='exponential', max_depth=4,\n",
    "                                max_features='auto', min_samples_leaf=60, min_samples_split=8,\n",
    "                                n_estimators=470, subsample=0.5,\n",
    "                                random_state=42).fit(X_train, y_train)\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_test), y_test, average='micro'))\n",
    "print(\"f1_score: \" , f1_score(gb.predict(X_train), y_train, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
